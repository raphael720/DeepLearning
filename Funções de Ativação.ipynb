{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Linear](#Linear)\n",
    "- [Sigmoid](#Sigmoid)\n",
    "- [Tanh](#Tanh)\n",
    "- [Rectified Linear Unit (ReLU)](#Rectified-Linear-Unit-(ReLU))\n",
    "- [Leaky ReLU](#Leaky-ReLU)\n",
    "- [Exponential Linear Unit (eLU)](#Exponential-Linear-Unit-(eLU))\n",
    "- [Tabela das Funções de Ativação](#Tabela-das-Funções-de-Ativação)\n",
    "- [Referências](#Referências)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Objetivos__:\n",
    "\n",
    "- Implementar as principais funções de ativação\n",
    "- Entender intuitivamente como $w$ e $b$ influenciam nas funções de ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as wg\n",
    "from ipywidgets import interactive, fixed\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interactive(w, b, func, ylim=fixed((0, 1)), show_der=False):\n",
    "    plt.figure(0)\n",
    "    \n",
    "    x = np.linspace(-10, 10, num=1000)\n",
    "    z = w*x + b\n",
    "    y = func(z)\n",
    "    \n",
    "    plt.plot(x, y, color='blue')\n",
    "    if show_der:\n",
    "        der = func(z, derivative=True)\n",
    "        y_der_z = der\n",
    "        y_der_x = w*der\n",
    "        plt.plot(x, y_der_z, color='red')\n",
    "        plt.plot(x, y_der_x, color='green')\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(ylim[0], ylim[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obs: \n",
    "- O grafico em azul é o plot da função\n",
    "- Em vermelho é a derivada da função\n",
    "- Em verde é a derivada da função sendo o x = w*x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $y \\in \\left [ +\\infty, -\\infty \\right ]$\n",
    "- Ela é a função de ativação mais simples\n",
    "- Utilizada na camada de saida de um problema de regrassão\n",
    "- Possui uma baixa complexidade\n",
    "- Baixo poder de aprendizagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y=x$$\n",
    "\n",
    "$$y^\\prime = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, derivative=False):\n",
    "    return np.ones_like(x) if derivative else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de777a45456f475797cc1cae44a1e556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='w', max=2.0, min=-2.0), FloatSlider(value=0.0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_plot = interactive(plot_interactive, w=(-2.0, 2.0), b=(-3, 3, 0.5), func=fixed(linear), ylim=fixed((-10, 10)))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $y \\in \\left ( 0, 1 \\right)$\n",
    "- Tambem conhecida como função logistica\n",
    "- Muito utilizada em problemas de regressão logistica(classificação)\n",
    "- A saida não é centrada em zero, por isso atrapalha um pouco o aprendizado\n",
    "- Não utilizar em problemas complexos\n",
    "- Satura os gradientes\n",
    "- Convergem lentamente, isso vai acontecer por causa dos seus gradientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "$$y^\\prime = y(1-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    if derivative:\n",
    "        y = sigmoid(x)\n",
    "        return y*(1-y)\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb81452676d44b2dac9abc0055e1e6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='w', max=2.0, min=-2.0), FloatSlider(value=0.0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_plot = interactive(plot_interactive, w=(-2.0, 2.0), b=(-3, 3, 0.5), func=fixed(sigmoid))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $y \\in \\left ( 0, 1 \\right)$\n",
    "- Se você prestar atenção ela é uma versão escalonada da sigmoid\n",
    "- Saida centrada em zero\n",
    "- Também vai satura os gradiente, porém menos que a sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = \\frac{e^x - e^{-x}}{e^x+e^{-x}}$$\n",
    "\n",
    "$$y^\\prime = 1 - y^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, derivative=False):\n",
    "    if derivative:\n",
    "        y = tanh(x)\n",
    "        return 1 - y**2\n",
    "    return (np.exp(x) - np.exp(-x)) /(np.exp(x) + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9014af8da714fb19c684a1460872e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='w', max=2.0, min=-2.0), FloatSlider(value=0.0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_plot = interactive(plot_interactive, w=(-2.0, 2.0), b=(-3, 3, 0.5), func=fixed(tanh), ylim=fixed((-2, 2)))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rectified Linear Unit (ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $y \\in \\left [ 0, +\\infty \\right)$\n",
    "- É a função de ativação mais utilizada\n",
    "- Simples e eficiente\n",
    "- Ajuda a evitar o problema de saturação dos gradientes \n",
    "- Com gradiente descendente converge mais rapido em comparação com a Sigmoid/Tanh\n",
    "- Só deve ser utilizada nas camdas escondidas\n",
    "- Ela pode matar os neurônios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = max(0, x)$$\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x} = \\begin{cases}0 &,\\  x \\leq 0\\\\1 &,\\ x > 0\\end{cases}$$\n",
    "\n",
    "__Obs.__: Lembrando que a derivada da ReLU quando x = 0 não existe matematicamente, mas é convencionalmente definida como 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return np.where(x <= 0, 0, 1)\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a7dec7e6da4c8c861b072ee5f05296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='w', max=2.0, min=-2.0), FloatSlider(value=0.0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_plot = interactive(plot_interactive, w=(-2.0, 2.0), b=(-3, 3, 0.5), func=fixed(relu), ylim=fixed((-1, 10)))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaky ReLU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $y \\in \\left( -\\infty, +\\infty \\right)$\n",
    "- Ela é uma pequena modificação da ReLu\n",
    "- Diminui a chance de dead units\n",
    "- Só deve ser utilizada nas camadas escondidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = \\begin{cases}\\alpha x &,\\ x \\leq 0\\\\x &,\\ x > 0\\end{cases}$$\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x} = \\begin{cases}\\alpha &,\\  x \\leq 0\\\\1 &,\\  x > 0\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x, derivative=False):\n",
    "    alpha = 0.1\n",
    "    if derivative:\n",
    "        return np.where(x <= 0, alpha, 1)\n",
    "    return np.where(x <= 0, alpha*x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d368e59b773642f085b52d6c9ccef10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='w', max=2.0, min=-2.0), FloatSlider(value=0.0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_plot = interactive(plot_interactive, w=(-2.0, 2.0), b=(-3, 3, 0.5), func=fixed(leaky_relu), ylim=fixed((-1, 10)))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential Linear Unit (eLU) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $y \\in \\left( -\\alpha, +\\infty \\right ]$\n",
    "- Ela é uma pequena modificação da ReLu\n",
    "- Diminui a chance de dead units\n",
    "- Só deve ser utilizada nas camadas escondidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = \\begin{cases}\\alpha(e^x -1) &,\\ x \\leq 0\\\\x &,\\ x > 0\\end{cases}$$\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x} = \\begin{cases}y + \\alpha &,\\  x \\leq 0\\\\1 &,\\  x > 0\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(x, derivative=False):\n",
    "    alpha = 0.5\n",
    "    if derivative:\n",
    "        y = elu(x)\n",
    "        return np.where(x <= 0, y + alpha, 1)\n",
    "    return np.where(x <= 0, alpha*(np.exp(x)-1), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14821cf01c194856b870c502fa09634a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='w', max=2.0, min=-2.0), FloatSlider(value=0.0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_plot = interactive(plot_interactive, w=(-2.0, 2.0), b=(-3, 3, 0.5), func=fixed(elu), ylim=fixed((-2, 10)))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabela das Funções de Ativação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imagens/funcoes_de_ativacao.png\" width='700'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Tabela das funções de ativação](https://en.wikipedia.org/wiki/Activation_function)\n",
    "- [Towards Data Science](https://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6)\n",
    "- [Stack Exchange](https://stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "state": {
    "568cf18b31b04b14b5b59379918b64e2": {
     "views": [
      {
       "cell_index": 6
      }
     ]
    },
    "58032045773043aeb5dab953be590b44": {
     "views": [
      {
       "cell_index": 30
      }
     ]
    },
    "743ccf7c641c485e9250b2558be149d1": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "a41e4be364004b0d924d05e304ee3bcb": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "bbf702b6ba494528a2233c691e2bd6d9": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "c259af61fd66413783ae3ed1e32ece47": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "c7f3128cf6d8492795e406ed70fa7e07": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
